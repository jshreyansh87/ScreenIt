{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linguistics\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# math tools\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#visual and file handeling tools\n",
    "import pickle\n",
    "import fitz\n",
    "import requests\n",
    "from bs4.element import Tag\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "#system tools\n",
    "import ast\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from rich import print as prt\n",
    "import clipboard as cb\n",
    "\n",
    "#data\n",
    "from train_data.cleaned_train_dataset_2 import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    # convert tuples to list and return converted data\n",
    "    def prepareData(train_data):\n",
    "        data = []\n",
    "        for text, annot in train_data:\n",
    "            ent = []\n",
    "            for strt, end, lbl in annot['entities']:\n",
    "                ent.append([strt, end, lbl])\n",
    "            annot['entities'] = ent\n",
    "            data.append([text, annot])\n",
    "        return data\n",
    "\n",
    "    # visualise training data using displacy\n",
    "    def renderData(data, n=0, serve=False):\n",
    "        nlp = spacy.blank('en')\n",
    "        data0 = [data[n]]\n",
    "        for text, annotations in data0:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if(span!=None):\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "        if serve:\n",
    "            displacy.serve(doc, style='ent')\n",
    "        else:\n",
    "            displacy.render(doc, style='ent')\n",
    "\n",
    "\n",
    "    '''\n",
    "        convert training dataset from v2 to v3 using docbin\n",
    "        note use filter_span to to get rid of the span errors\n",
    "    '''\n",
    "    def v2Tov3Converter(data, filename=\"train\", rigrousFilter=False):\n",
    "        nlp = spacy.blank(\"en\") \n",
    "        db = DocBin() # DocBin will store the example documents\n",
    "        for text, annotations in data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations['entities']:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span == None:\n",
    "                    continue\n",
    "                    \n",
    "                if(rigrousFilter):  # rigrously filter the spans.\n",
    "                    if span.text.isspace()==True:\n",
    "                        continue\n",
    "                    proceed = True\n",
    "                    span_text = span.text\n",
    "                    if span_text[0]==' ' or span_text[len(span_text)-1]==' ':\n",
    "                        continue\n",
    "                    for char in span.text:\n",
    "                        if char.isalnum():\n",
    "                            continue\n",
    "                        else:\n",
    "                            proceed = False\n",
    "\n",
    "                    if proceed:\n",
    "                        ents.append(span)\n",
    "\n",
    "                ents.append(span)\n",
    "\n",
    "                if(rigrousFilter):\n",
    "                    ents = filter_spans(ents)\n",
    "                \n",
    "            doc.ents = ents\n",
    "            doc = utils.remove_whitespace_entities(doc)\n",
    "            db.add(doc)\n",
    "        filename=filename+\".spacy\"\n",
    "        db.to_disk(filename)\n",
    "        return list(db.get_docs(nlp.vocab))\n",
    "    \n",
    "    def initializeConfig():\n",
    "        # initialize config.cfg file for ner training\n",
    "        os.system(\"spacy init config --lang en --pipeline ner config.cfg --force\")\n",
    "    \n",
    "    def trainModel():\n",
    "        # train the model using config.cfg file. and save the model in trained_model folder.\n",
    "        os.system(\"spacy train config.cfg --output ./trained_model/ --paths.train ./train.spacy --paths.dev ./train.spacy\")\n",
    "    \n",
    "    def loadTrainData():\n",
    "        train_data = pickle.load(open('train_data/train_data.pkl', 'rb'))\n",
    "        return train_data\n",
    "    \n",
    "    def prepareDocSpansHtml(doc):\n",
    "        text = doc[0]\n",
    "        annotations = doc[1]['entities']\n",
    "        md = \"\"\n",
    "        switcher = 0\n",
    "        for i in range(len(text)+1):\n",
    "            for j in annotations:\n",
    "                if j[0] == i:\n",
    "                    if switcher:\n",
    "                        md+='<span style=\"color: red;\">*'\n",
    "                    else:\n",
    "                        md+='<span style=\"color: green;\">*'\n",
    "                    switcher = not switcher\n",
    "                if j[1] == i:\n",
    "                    md+='*</span>'\n",
    "            if i<len(text):\n",
    "                md+=text[i]\n",
    "        cb.copy(md)\n",
    "        md = Markdown(md)\n",
    "\n",
    "    def printSpans(data):\n",
    "        for i in data[1]['entities']:\n",
    "            prt(data[0][i[0]: i[1]], \" :\", i[2])\n",
    "    \n",
    "    def printErrorSpans(dataset):\n",
    "        ar = []\n",
    "        for i in range(len(dataset)):\n",
    "            anno = dataset[i][1]['entities']\n",
    "            for j in anno:\n",
    "                span = dataset[i][0][j[0]:j[1]]\n",
    "                l = len(span)\n",
    "                if(l>0):\n",
    "                    if(span[0]==' ' or span[l-1]==' '):\n",
    "                        prt(span, i)\n",
    "                        if i not in ar:\n",
    "                            ar.append(i)\n",
    "                if(l<1):\n",
    "                    prt(\"[bold green]0 length span[/bold green] - \" ,i, dataset[i][0])\n",
    "        prt(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading and tesing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pl.Path('../meta_data/forbes_2000/forbes_2000.csv')\n",
    "forbes_companies = pd.read_csv(path)\n",
    "for i in range(7):\n",
    "    forbes_companies = forbes_companies.drop(index=i)\n",
    "col = list(forbes_companies.columns)\n",
    "col[2] = 'company name'\n",
    "forbes_companies.columns = col\n",
    "forbes_companies = list(forbes_companies['company name'])\n",
    "forbes_companies = [i.lower() for i in forbes_companies]\n",
    "forbes_companies = [j.split(\" \")[0] for j in forbes_companies]\n",
    "forbes_companies = list(dict.fromkeys(forbes_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class webScrapper:\n",
    "    def getGoogleSearchResults(query)->list:\n",
    "        results = list(search(query, tld=\"co.in\", num=1, stop=1, pause=0.5))\n",
    "        return results\n",
    "\n",
    "    def getPageText(url):\n",
    "        result = requests.get(url)\n",
    "        html_text = result.content\n",
    "        soup = BeautifulSoup(html_text)\n",
    "\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.decompose()\n",
    "        strips = list(soup.stripped_strings)\n",
    "        final_string = ''\n",
    "        for i in strips:\n",
    "            if len(i)>100:\n",
    "                final_string+=i\n",
    "                final_string+=\" \"\n",
    "        return final_string\n",
    "\n",
    "    def getText(query):\n",
    "        search_res = webScrapper.getGoogleSearchResults(query)\n",
    "        text = webScrapper.getPageText(search_res[0])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class score_parameter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scemantic_model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "    def getScemanticSimilarity(self, sentence1, sentence2):\n",
    "        # encode sentences to get their embeddings\n",
    "        embedding1 = self.scemantic_model.encode(sentence1, convert_to_tensor=True)\n",
    "        embedding2 = self.scemantic_model.encode(sentence2, convert_to_tensor=True)\n",
    "        \n",
    "        # compute similarity scores of two embeddings\n",
    "        cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "        return cosine_scores.item()\n",
    "\n",
    "    def getResumeFitness(self, spans, jdtext):\n",
    "        \n",
    "        scores = []\n",
    "        if('designation' in spans.keys()):\n",
    "            if(len(spans['designation'])>5):\n",
    "                desg = spans['designation'][:5]\n",
    "            else:\n",
    "                desg = spans['designation']\n",
    "            for i in desg:\n",
    "                text = webScrapper.getText(i)\n",
    "                scores.append(score_parameter.getScemanticSimilarity(self, sentence1=text, sentence2=jdtext))\n",
    "        if (len(scores)>0):\n",
    "            return statistics.mean(scores)*100\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    \n",
    "    def getConfidenceScore(self, text):\n",
    "        return TextBlob(text).sentiment\n",
    "    \n",
    "\n",
    "    def jobSteadynessScore(self, spans):\n",
    "        # longer one stays in one company->better it is\n",
    "        optimum_duration = 5\n",
    "        work_duration = 0\n",
    "\n",
    "        if('total experience' in spans.keys()):\n",
    "            ar = (spans['total experience'][0])\n",
    "            count = 0\n",
    "            str = ''\n",
    "            nums = []\n",
    "            for i in ar:\n",
    "                if(((ord(i)>=48 and ord(i)<=57) or i=='.') and count<2):\n",
    "                    if(i!='.'):\n",
    "                        count+=1\n",
    "                    str+=i\n",
    "                if(i==' ' and str.isalnum()):\n",
    "                    nums.append(float(str))\n",
    "                    str = ''\n",
    "            if(len(nums)>1):\n",
    "                work_duration = nums[0]+nums[1]/12\n",
    "            elif(len(nums)>0):\n",
    "                work_duration = nums[0]\n",
    "            elif(str.isalnum()):\n",
    "                work_duration = float(str)\n",
    "\n",
    "        elif('experience duration' in spans.keys()):\n",
    "            ar = spans['experience duration']\n",
    "            for j in ar:\n",
    "                count = 0\n",
    "                str = ''\n",
    "                for i in j:\n",
    "                    if(((ord(i)>=48 and ord(i)<=57) or i=='.') and count<8):\n",
    "                        count+=1\n",
    "                        str+=i\n",
    "                d1 = str[:4]\n",
    "                d2 = str[4:]\n",
    "                if(count==8):\n",
    "                    d1 = int(d1)\n",
    "                    d2 = int(d2)\n",
    "                    work_duration+=abs(d1-d2)\n",
    "        ans = (work_duration/optimum_duration)*100\n",
    "        if(ans<100):\n",
    "            return ans\n",
    "        elif(ans>100):\n",
    "            return 100\n",
    "        elif(ans==0):\n",
    "            return random.randint(0, 30)\n",
    "\n",
    "\n",
    "    def orgProfileScore(self, spans):\n",
    "        #return the number of forbes companies worked at.\n",
    "        orgCount=0\n",
    "        for i in spans['companies worked at']:\n",
    "            for j in i.split(\" \"):\n",
    "                if(j.lower() in forbes_companies):\n",
    "                    orgCount+=1\n",
    "        return orgCount\n",
    "\n",
    "    def certificationScore(self, spans):\n",
    "        if ('certification' in spans.keys()):\n",
    "            return len(spans['certification'])\n",
    "        return 0\n",
    "\n",
    "    def winsScore(self, spans):\n",
    "        wins = 0\n",
    "        if('wins' in spans.keys()):\n",
    "            wins = len(spans['wins'])\n",
    "        return wins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'companies worked at': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)',\n",
    "}\n",
    "options = {\n",
    "    'ents': ['companies worked at'], \"colors\": colors\n",
    "}\n",
    "\n",
    "class tools:\n",
    "    def loadPdfs(dir='../meta_data/resume and jd/resumes'):\n",
    "        path = pl.Path(dir)\n",
    "        return list(path.glob(\"*.pdf\"))\n",
    "\n",
    "    def extractTextFromPdf(path, pdf_number):\n",
    "        pdf = fitz.open(path[pdf_number])\n",
    "        text = ''\n",
    "        for page in pdf:\n",
    "            text += str(page.getText())\n",
    "        text = \" \".join(text.split('\\n'))\n",
    "        return text\n",
    "    \n",
    "    def loadTextData():\n",
    "        texts = [i[0] for i in dataset]\n",
    "        return texts\n",
    "\n",
    "\n",
    "class linguistics(tools):\n",
    "    def __init__(self, last=True) -> None:\n",
    "        if(last):\n",
    "            model_dir = './trained_model/model-last'\n",
    "        else:\n",
    "            model_dir = './trained_model/model-best'\n",
    "        self.model = spacy.load(model_dir)\n",
    "        self.parameters = score_parameter()\n",
    "    \n",
    "\n",
    "    def render(self, text):\n",
    "        displacy.render(self.model(text), style='ent')\n",
    "\n",
    "    \n",
    "    def getUniqueSpans(self, text)->dict:\n",
    "        doc = self.model(text)\n",
    "        labels = dict()\n",
    "        for i in doc.ents:\n",
    "            if i.label_ not in labels.keys():\n",
    "                labels[i.label_] = [i.text]\n",
    "            else:\n",
    "                labels[i.label_].append(i.text)\n",
    "        for i in labels.keys():\n",
    "            labels[i] = list(set(labels[i]))\n",
    "        return labels\n",
    "\n",
    "\n",
    "    def printEntitySpansAndLabels(self, text):\n",
    "        doc = self.model(text)\n",
    "        for i in doc.ents:\n",
    "            print(i, \" | \", i.label_)\n",
    "\n",
    "\n",
    "    def getAllScores(self, text, jdtext)->dict:\n",
    "        spans = self.getUniqueSpans(text)\n",
    "        try:\n",
    "            prt(\"resume fitness score: \", self.parameters.getResumeFitness(spans, jdtext))\n",
    "        except:\n",
    "            prt('failed to calculate resume fitness score')\n",
    "        \n",
    "\n",
    "        try:\n",
    "            prt(\"organization profile score: \", self.parameters.orgProfileScore(spans))\n",
    "        except:\n",
    "            prt('failed to calculate resume profile score')\n",
    "\n",
    "\n",
    "        try:\n",
    "            prt(\"job steadyness score: \", self.parameters.jobSteadynessScore(spans))\n",
    "        except:\n",
    "            prt('failed to calculate resume job steadyness score')\n",
    "\n",
    "\n",
    "        try:\n",
    "            prt(\"wins score: \", self.parameters.winsScore(spans))\n",
    "        except:\n",
    "            prt('failed to calculate resume wins score')\n",
    "\n",
    "\n",
    "        try:\n",
    "            prt(\"certification score: \", self.parameters.certificationScore(spans))\n",
    "        except:\n",
    "            prt('failed to calculate resume certification score')\n",
    "\n",
    "\n",
    "        try:\n",
    "            prt(\"confidence level: \", self.parameters.getConfidenceScore(text))\n",
    "        except:\n",
    "            prt('failed to calculate resume confidence level')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linguistics()\n",
    "text_files = tools.loadTextData()\n",
    "\n",
    "jdPath = \"../meta_data/resume and jd/jd\"\n",
    "pdfs = tools.loadPdfs(jdPath)\n",
    "jdtext = tools.extractTextFromPdf(pdfs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">resume fitness score:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.94068622589111</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "resume fitness score:  \u001b[1;36m29.94068622589111\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">organization profile score:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "organization profile score:  \u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">job steadyness score:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "job steadyness score:  \u001b[1;36m100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">wins score:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "wins score:  \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">certification score:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "certification score:  \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">failed to calculate resume confidence level\n",
       "</pre>\n"
      ],
      "text/plain": [
       "failed to calculate resume confidence level\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.getAllScores(text_files[1], tools.extractTextFromPdf(pdfs, 2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

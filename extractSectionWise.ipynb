{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>logical approach to extract named entities</b>\n",
    "\n",
    "### approach - divide the file in 4 sections and find the entities based on predifined list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, fitz\n",
    "import pathlib as pl\n",
    "import spacy\n",
    "import spacy.displacy as displacy\n",
    "from operator import itemgetter\n",
    "# from section import *\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = pl.Path('./dataset/Resume&Job_Description/Original_Resumes')\n",
    "resumes = list(resumes.glob('*.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = fitz.open(resumes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading sample dataset\n",
    "\n",
    "text = ''\n",
    "for page in pdf:\n",
    "    text = text + str(page.getText())\n",
    "text = \" \".join(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fonts(doc, granularity=False):\n",
    "    \"\"\"Extracts fonts and their usage in PDF documents.\n",
    "    :param doc: PDF document to iterate through\n",
    "    :type doc: <class 'fitz.fitz.Document'>\n",
    "    :param granularity: also use 'font', 'flags' and 'color' to discriminate text\n",
    "    :type granularity: bool\n",
    "    :rtype: [(font_size, count), (font_size, count}], dict\n",
    "    :return: most used fonts sorted by count, font style information\n",
    "    \"\"\"\n",
    "    styles = {}\n",
    "    font_counts = {}\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.getText(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:  # iterate through the text blocks\n",
    "            if b['type'] == 0:  # block contains text\n",
    "                for l in b[\"lines\"]:  # iterate through the text lines\n",
    "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                        if granularity:\n",
    "                            identifier = \"{0}_{1}_{2}_{3}\".format(s['size'], s['flags'], s['font'], s['color'])\n",
    "                            styles[identifier] = {'size': s['size'], 'flags': s['flags'], 'font': s['font'],\n",
    "                                                  'color': s['color']}\n",
    "                        else:\n",
    "                            identifier = \"{0}\".format(s['size'])\n",
    "                            styles[identifier] = {'size': s['size'], 'font': s['font']}\n",
    "\n",
    "                        font_counts[identifier] = font_counts.get(identifier, 0) + 1  # count the fonts usage\n",
    "\n",
    "    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if len(font_counts) < 1:\n",
    "        raise ValueError(\"Zero discriminating fonts found!\")\n",
    "\n",
    "    return font_counts, styles\n",
    "\n",
    "\n",
    "def font_tags(font_counts, styles):\n",
    "    \"\"\"Returns dictionary with font sizes as keys and tags as value.\n",
    "    :param font_counts: (font_size, count) for all fonts occuring in document\n",
    "    :type font_counts: list\n",
    "    :param styles: all styles found in the document\n",
    "    :type styles: dict\n",
    "    :rtype: dict\n",
    "    :return: all element tags based on font-sizes\n",
    "    \"\"\"\n",
    "    p_style = styles[font_counts[0][0]]  # get style for most used font by count (paragraph)\n",
    "    p_size = p_style['size']  # get the paragraph's size\n",
    "\n",
    "    # sorting the font sizes high to low, so that we can append the right integer to each tag \n",
    "    font_sizes = []\n",
    "    for (font_size, count) in font_counts:\n",
    "        font_sizes.append(float(font_size))\n",
    "    font_sizes.sort(reverse=True)\n",
    "\n",
    "    # aggregating the tags for each font size\n",
    "    idx = 0\n",
    "    size_tag = {}\n",
    "    for size in font_sizes:\n",
    "        idx += 1\n",
    "        if size == p_size:\n",
    "            idx = 0\n",
    "            size_tag[size] = '<p>'\n",
    "        if size > p_size:\n",
    "            size_tag[size] = '<h{0}>'.format(idx)\n",
    "        elif size < p_size:\n",
    "            size_tag[size] = '<s{0}>'.format(idx)\n",
    "\n",
    "    return size_tag\n",
    "\n",
    "\n",
    "def headers_para(doc, size_tag):\n",
    "    \"\"\"Scrapes headers & paragraphs from PDF and return texts with element tags.\n",
    "    :param doc: PDF document to iterate through\n",
    "    :type doc: <class 'fitz.fitz.Document'>\n",
    "    :param size_tag: textual element tags for each size\n",
    "    :type size_tag: dict\n",
    "    :rtype: list\n",
    "    :return: texts with pre-prended element tags\n",
    "    \"\"\"\n",
    "    header_para = []  # list with headers and paragraphs\n",
    "    first = True  # boolean operator for first header\n",
    "    previous_s = {}  # previous span\n",
    "\n",
    "    for page in doc:\n",
    "        blocks = page.getText(\"dict\")[\"blocks\"]\n",
    "        for b in blocks:  # iterate through the text blocks\n",
    "            if b['type'] == 0:  # this block contains text\n",
    "\n",
    "                # REMEMBER: multiple fonts and sizes are possible IN one block\n",
    "\n",
    "                block_string = \"\"  # text found in block\n",
    "                for l in b[\"lines\"]:  # iterate through the text lines\n",
    "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
    "                        if s['text'].strip():  # removing whitespaces:\n",
    "                            if first:\n",
    "                                previous_s = s\n",
    "                                first = False\n",
    "                                block_string = size_tag[s['size']] + s['text']\n",
    "                            else:\n",
    "                                if s['size'] == previous_s['size']:\n",
    "\n",
    "                                    if block_string and all((c == \"|\") for c in block_string):\n",
    "                                        # block_string only contains pipes\n",
    "                                        block_string = size_tag[s['size']] + s['text']\n",
    "                                    if block_string == \"\":\n",
    "                                        # new block has started, so append size tag\n",
    "                                        block_string = size_tag[s['size']] + s['text']\n",
    "                                    else:  # in the same block, so concatenate strings\n",
    "                                        block_string += \" \" + s['text']\n",
    "\n",
    "                                else:\n",
    "                                    header_para.append(block_string)\n",
    "                                    block_string = size_tag[s['size']] + s['text']\n",
    "\n",
    "                                previous_s = s\n",
    "\n",
    "                    # new block started, indicating with a pipe\n",
    "                    block_string += \"|\"\n",
    "\n",
    "                header_para.append(block_string)\n",
    "\n",
    "    return header_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21.959999084472656_16_Calibri-Bold_16711680': {'size': 21.959999084472656,\n",
       "  'flags': 16,\n",
       "  'font': 'Calibri-Bold',\n",
       "  'color': 16711680},\n",
       " '21.959999084472656_16_Calibri-Bold_0': {'size': 21.959999084472656,\n",
       "  'flags': 16,\n",
       "  'font': 'Calibri-Bold',\n",
       "  'color': 0},\n",
       " '12.0_16_Calibri-Bold_0': {'size': 12.0,\n",
       "  'flags': 16,\n",
       "  'font': 'Calibri-Bold',\n",
       "  'color': 0},\n",
       " '8.039999961853027_17_Calibri-Bold_0': {'size': 8.039999961853027,\n",
       "  'flags': 17,\n",
       "  'font': 'Calibri-Bold',\n",
       "  'color': 0},\n",
       " '11.039999961853027_0_Calibri_0': {'size': 11.039999961853027,\n",
       "  'flags': 0,\n",
       "  'font': 'Calibri',\n",
       "  'color': 0},\n",
       " '12.0_16_Calibri-Bold_16711680': {'size': 12.0,\n",
       "  'flags': 16,\n",
       "  'font': 'Calibri-Bold',\n",
       "  'color': 16711680},\n",
       " '12.0_0_Calibri_0': {'size': 12.0, 'flags': 0, 'font': 'Calibri', 'color': 0},\n",
       " '11.039999961853027_0_Calibri_16711680': {'size': 11.039999961853027,\n",
       "  'flags': 0,\n",
       "  'font': 'Calibri',\n",
       "  'color': 16711680},\n",
       " '11.039999961853027_0_SymbolMT_0': {'size': 11.039999961853027,\n",
       "  'flags': 0,\n",
       "  'font': 'SymbolMT',\n",
       "  'color': 0},\n",
       " '11.039999961853027_0_ArialMT_0': {'size': 11.039999961853027,\n",
       "  'flags': 0,\n",
       "  'font': 'ArialMT',\n",
       "  'color': 0},\n",
       " '12.0_0_SymbolMT_0': {'size': 12.0,\n",
       "  'flags': 0,\n",
       "  'font': 'SymbolMT',\n",
       "  'color': 0},\n",
       " '12.0_0_ArialMT_0': {'size': 12.0, 'flags': 0, 'font': 'ArialMT', 'color': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-31f5752bc725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mskills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskills\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: T001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-31f5752bc725>\u001b[0m in \u001b[0;36mextract_skills\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "    'html',\n",
    "    'css'\n",
    "]\n",
    "\n",
    "\n",
    "# def extract_text_from_docx(docx_path):\n",
    "#     txt = docx2txt.process(docx_path)\n",
    "#     if txt:\n",
    "#         return txt.replace('\\t', ' ')\n",
    "#     return None\n",
    "\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "\n",
    "skills = extract_skills(text)\n",
    "\n",
    "print(skills)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
